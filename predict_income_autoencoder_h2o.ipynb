{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.deeplearning import H2OAutoEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.data_wrangling as dw\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import avg, array\n",
    "\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import svm\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import svm\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import svm\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and get column names\n",
    "\n",
    "data = pd.read_csv('../predict_income.csv')\n",
    "\n",
    "data.shape\n",
    "\n",
    "# data with outlier detection values\n",
    "\n",
    "data_w_outlier_val = data.copy()\n",
    "\n",
    "# outcome variable label\n",
    "\n",
    "outcome = 'income'\n",
    "\n",
    "# id column label\n",
    "\n",
    "id_col = 'id'\n",
    "\n",
    "# list of categorical columns\n",
    "\n",
    "# obtained from kaggle website\n",
    "cat_col = ['workclass', 'education',\n",
    "           'marital-status', 'occupation', \n",
    "           'relationship',\n",
    "           'race', 'sex', \n",
    "           'native-country', 'income']\n",
    "\n",
    "# categorical columns without outcome variable\n",
    "\n",
    "cat_col_wo_outcome = [col for col in cat_col if col != outcome]\n",
    "\n",
    "# cat cols without outcome and two category values\n",
    "\n",
    "cat_col_more_than_2 = list()\n",
    "for col in cat_col:\n",
    "    if data[col].nunique() > 2:\n",
    "        cat_col_more_than_2.append(col)\n",
    "\n",
    "cat_col_2_or_fewer = list(set(cat_col) - set(cat_col_more_than_2))\n",
    "\n",
    "# list of non-id numeric columns\n",
    "\n",
    "num_col = [col for col in data.columns if col not in cat_col and col != id_col]\n",
    "\n",
    "# Convert categorical columns to \"categorical\" in pandas\n",
    "\n",
    "data_with_cat = dw.convert_col_to_cat(data, cat_col)\n",
    "\n",
    "data_with_cat.head()\n",
    "\n",
    "# equal width discretization of numeric columns\n",
    "\n",
    "data_with_cat = dw.convert_num_to_obj(data_with_cat, num_col)\n",
    "\n",
    "# Attribute value frequency\n",
    "\n",
    "## read in pandas df as spark df\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data_cat_only_spark_df = spark.createDataFrame(data_with_cat)\n",
    "\n",
    "for column in data_cat_only_spark_df.columns:\n",
    "    if column not in ['id', 'income']:\n",
    "        data_cat_only_spark_df = data_cat_only_spark_df.withColumn(column, count(column).over(Window.partitionBy(column)))\n",
    "\n",
    "avg_cols = udf(lambda array: sum(array)/len(array), DoubleType())\n",
    "\n",
    "wanted_cols = [col for col in data_cat_only_spark_df.columns if col not in ['id', 'income']]\n",
    "\n",
    "data_cat_only_spark_df_with_avf = data_cat_only_spark_df.withColumn(\"avf\", avg_cols(array(*wanted_cols)))\n",
    "\n",
    "# ## count frequency for each category\n",
    "\n",
    "# def count_freq_for_cat(df):\n",
    "#     counts_dict = dict()\n",
    "#     for col in df.select_dtypes(['category']):\n",
    "#         counts_dict[col] = df[col].value_counts().to_dict()\n",
    "#     return counts_dict\n",
    "\n",
    "# counts_dict = count_freq_for_cat(data_with_cat)\n",
    "\n",
    "# ## use map in pandas\n",
    "\n",
    "# def map_freq_to_value(df, counts_dict):\n",
    "#     df_with_freq = pd.DataFrame()\n",
    "#     for col in df:\n",
    "#         if str(df[col].dtypes) == 'category' and col != outcome:\n",
    "#             df_with_freq[col] = df[col].map(counts_dict[col])\n",
    "#         else:\n",
    "#             df_with_freq[col] = df[col]\n",
    "#     return df_with_freq\n",
    "\n",
    "# data_num_to_cat_with_freq = map_freq_to_value(data_with_cat, counts_dict)\n",
    "\n",
    "# data_num_to_cat_with_freq.head()\n",
    "\n",
    "# data_num_to_cat_with_freq['avf'] = data_num_to_cat_with_freq.drop(columns=[outcome, id_col]).apply(np.sum, axis=1) / 13\n",
    "\n",
    "data_cat_only_pd_with_avf = data_cat_only_spark_df_with_avf.toPandas()\n",
    "\n",
    "data_cat_only_pd_with_avf[['id', 'avf', outcome]].head()\n",
    "\n",
    "## add avf to data_w_outlier_val\n",
    "\n",
    "data_w_outlier_val = data_w_outlier_val.merge(data_cat_only_pd_with_avf[['id', 'avf']], on='id')\n",
    "\n",
    "# Local Outlier Factor\n",
    "\n",
    "data_all_num = dw.convert_cat_to_numeric(data, cat_col_more_than_2, cat_col_2_or_fewer)\n",
    "\n",
    "## drop unwanted variables\n",
    "\n",
    "X = data_all_num.drop(columns=[outcome])\n",
    "\n",
    "## fit predict local outlier factor\n",
    "\n",
    "lof = LocalOutlierFactor(n_jobs=-1)\n",
    "\n",
    "y_pred = lof.fit_predict(X.drop(columns=id_col))\n",
    "\n",
    "len(lof.negative_outlier_factor_)\n",
    "\n",
    "lof.negative_outlier_factor_ * -1\n",
    "\n",
    "## make copy of X dataframe\n",
    "\n",
    "X_lof = X.copy()\n",
    "\n",
    "## add lof values to X dataframe\n",
    "\n",
    "X_lof['lof'] = lof.negative_outlier_factor_ * -1\n",
    "\n",
    "X_lof.head()\n",
    "\n",
    "## add lof to data_w_outlier_val\n",
    "\n",
    "data_w_outlier_val = data_w_outlier_val.merge(X_lof[['id', 'lof']], on='id')\n",
    "\n",
    "data_w_outlier_val.head(100)\n",
    "\n",
    "# one-class svm\n",
    "\n",
    "## copy X dataframe to X_svm\n",
    "\n",
    "X_svm = X.copy()\n",
    "\n",
    "## standardize data for svm\n",
    "\n",
    "X_standardized = dw.standardize_numeric_variables(X_svm.drop(columns=id_col), X_svm.columns[1:])\n",
    "\n",
    "X_standardized.head()\n",
    "\n",
    "## instantiate svm class\n",
    "\n",
    "clf = svm.OneClassSVM()\n",
    "\n",
    "## fit the data\n",
    "\n",
    "clf.fit(X_standardized)\n",
    "\n",
    "## get the predicted values\n",
    "\n",
    "X_svm['svm'] = clf.decision_function(X_standardized)\n",
    "\n",
    "# add svm results to data_w_outlier_val\n",
    "\n",
    "data_w_outlier_val = data_w_outlier_val.merge(X_svm[['id', 'svm']], on='id')\n",
    "\n",
    "data_w_outlier_val.head(100)\n",
    "\n",
    "# isolation forest\n",
    "\n",
    "X_standardized.shape\n",
    "\n",
    "clf = IsolationForest(max_samples=100)\n",
    "clf.fit(X_standardized)\n",
    "\n",
    "# predictions\n",
    "y_pred_train = clf.predict(X_standardized)\n",
    "\n",
    "data_w_outlier_val['iso_for'] = clf.decision_function(X_standardized)\n",
    "\n",
    "### The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.\n",
    "\n",
    "# autoencoder\n",
    "\n",
    "data_all_num = dw.convert_cat_to_numeric(data, cat_col_more_than_2, cat_col_2_or_fewer)\n",
    "\n",
    "num_data_standardized = dw.standardize_numeric_variables(data_all_num, data_all_num.columns)\n",
    "\n",
    "X_train = num_data_standardized.drop(columns=['id', 'income'])\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 14\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_train, X_train),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history\n",
    "\n",
    "autoencoder = load_model('model.h5')\n",
    "\n",
    "predictions = autoencoder.predict(X_train)\n",
    "\n",
    "mse = np.mean(np.power(X_train - predictions, 2), axis=1)\n",
    "\n",
    "sns.distplot(mse)\n",
    "\n",
    "mse.describe()\n",
    "\n",
    "data_w_outlier_val['autoencoder'] = mse\n",
    "\n",
    "# Anomaly Detection Scores\n",
    "\n",
    "def color(s):\n",
    "    row_color_list = list()\n",
    "\n",
    "    # avf\n",
    "    if s.avf < 1000:\n",
    "        row_color_list.extend(['background-color: red'])\n",
    "    elif s.avf >= 1000 and s.avf < 1200:\n",
    "        row_color_list.extend(['background-color: yellow'])\n",
    "    elif s.avf >= 1200:\n",
    "        row_color_list.extend(['background-color: green'])\n",
    "\n",
    "    # lof\n",
    "    if s.lof <= 1.05:\n",
    "        row_color_list.extend(['background-color: green'])\n",
    "    elif s.lof > 1.05 and s.lof < 1.3:\n",
    "        row_color_list.extend(['background-color: yellow'])\n",
    "    elif s.lof >= 1.3:\n",
    "        row_color_list.extend(['background-color: red'])\n",
    "\n",
    "    # svm\n",
    "    if s.svm < -100:\n",
    "        row_color_list.extend(['background-color: red'])\n",
    "    elif s.svm >= -100 and s.svm < 100:\n",
    "        row_color_list.extend(['background-color: yellow'])\n",
    "    elif s.svm >= 100:\n",
    "        row_color_list.extend(['background-color: green'])\n",
    "\n",
    "    # iso_for\n",
    "    if s.iso_for < -0.1:\n",
    "        row_color_list.extend(['background-color: red'])\n",
    "    elif s.iso_for >= -0.1 and s.iso_for < 0.1:\n",
    "        row_color_list.extend(['background-color: yellow'])\n",
    "    elif s.iso_for >= 0.1:\n",
    "        row_color_list.extend(['background-color: green'])\n",
    "\n",
    "    # autoencoder\n",
    "    if s.autoencoder > 0.396917:\n",
    "        row_color_list.extend(['background-color: red'])\n",
    "    elif s.autoencoder <= 0.396917 and s.autoencoder > 0.206993:\n",
    "        row_color_list.extend(['background-color: yellow'])\n",
    "    elif s.autoencoder <= 0.206993:\n",
    "        row_color_list.extend(['background-color: green'])\n",
    "\n",
    "    return row_color_list\n",
    "\n",
    "result_df = data_w_outlier_val.loc[:, ['id', 'income', 'avf', 'lof', 'svm', 'iso_for', 'autoencoder']]\n",
    "\n",
    "result_df_style = result_df.sort_values(by='income',\n",
    "                                        ascending=False).iloc[:1000, :1000].style.apply(color,\n",
    "                                                                                        subset=['avf',\n",
    "                                                                                               'lof',\n",
    "                                                                                               'svm',\n",
    "                                                                                               'iso_for',\n",
    "                                                                                               'autoencoder'],\n",
    "                                                                                        axis=1)\n",
    "\n",
    "result_df_style\n",
    "\n",
    "# 3d plots\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = data_all_num.loc[data_all_num[outcome] == 0, 'hours-per-week']\n",
    "xdata = data_all_num.loc[data_all_num[outcome] == 0, 'capital-loss']\n",
    "ydata = data_all_num.loc[data_all_num[outcome] == 0, 'capital-gain']\n",
    "ax.scatter3D(xdata, ydata, zdata, cmap=data_all_num[outcome], label='<=50K', alpha=0.5)\n",
    "zdata = data_all_num.loc[data_all_num[outcome] == 1, 'hours-per-week']\n",
    "xdata = data_all_num.loc[data_all_num[outcome] == 1, 'capital-loss']\n",
    "ydata = data_all_num.loc[data_all_num[outcome] == 1, 'capital-gain']\n",
    "ax.scatter3D(xdata, ydata, zdata, cmap=data_all_num[outcome], label='>50K', alpha=0.5)\n",
    "ax.set_xlabel('capital-loss')\n",
    "ax.set_ylabel('capital-gain')\n",
    "ax.set_zlabel('hours-per-week')\n",
    "ax.legend()\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = data_all_num.loc[data_all_num[outcome] == 0, 'hours-per-week']\n",
    "xdata = data_all_num.loc[data_all_num[outcome] == 0, 'age']\n",
    "ydata = data_all_num.loc[data_all_num[outcome] == 0, 'capital-gain']\n",
    "ax.scatter3D(xdata, ydata, zdata, cmap=data_all_num[outcome], label='<=50K', alpha=0.5)\n",
    "zdata = data_all_num.loc[data_all_num[outcome] == 1, 'hours-per-week']\n",
    "xdata = data_all_num.loc[data_all_num[outcome] == 1, 'age']\n",
    "ydata = data_all_num.loc[data_all_num[outcome] == 1, 'capital-gain']\n",
    "ax.scatter3D(xdata, ydata, zdata, cmap=data_all_num[outcome], label='>50K', alpha=0.5)\n",
    "ax.set_xlabel('age')\n",
    "ax.set_ylabel('capital-gain')\n",
    "ax.set_zlabel('hours-per-week')\n",
    "ax.legend()\n",
    "\n",
    "# generalized pairs plot\n",
    "\n",
    "data_cat_and_num = dw.convert_col_to_cat(data, cat_col)\n",
    "\n",
    "data_cat_and_num.dtypes\n",
    "\n",
    "data_cat_and_num.drop(columns='id', inplace=True)\n",
    "\n",
    "data_cat_num_subset = data_cat_and_num[['age', 'workclass', 'education', 'occupation', 'capital-gain']]\n",
    "\n",
    "col_names = data_cat_num_subset.columns\n",
    "\n",
    "import itertools\n",
    "col_pairs = [col for col in itertools.product(col_names, repeat=2)]\n",
    "\n",
    "col_pairs\n",
    "\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5)\n",
    "for i, ax_row in enumerate(axes):\n",
    "    for j, ax_col in enumerate(axes):\n",
    "        pair = col_pairs[count]\n",
    "        dtype_pair = list()\n",
    "        dtype_pair.append(str(data_cat_num_subset[pair[0]].dtypes))\n",
    "        dtype_pair.append(str(data_cat_num_subset[pair[1]].dtypes))\n",
    "        print(dtype_pair)\n",
    "        count += 1\n",
    "\n",
    "sns.boxplot(x='workclass',y='age',data=data_cat_and_num.loc[:, ['workclass', 'age']], ax=ax)\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(x='workclass',y='age',data=data_cat_and_num.loc[:, ['workclass', 'age']], ax=ax)\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x='age',y='capital-gain',data=data_cat_and_num, ax=ax)\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mosaic(data_cat_and_num, ['education', 'marital-status'], ax=ax, labelizer = lambda x: \"\")\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(a=data_cat_and_num['age'],  ax=ax)\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.countplot(x='race', data=data_cat_and_num,  ax=ax)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
